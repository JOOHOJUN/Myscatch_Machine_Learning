{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "조엘 그루스, 2016, 인사이트, '밑바닥부터 시작하는 데이터 과학'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from functools import partial\n",
    "import math, random\n",
    "from linear_algebra import dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#신경망 기본꼴\n",
    "def step_function(x):  # 활성함수\n",
    "    return 1 if x>= 0 else 0\n",
    "def perceptron_output(weights, bias, x):\n",
    "    calculation = dot(weights, x) + bias  # 선형분리 y = ax+b 꼴\n",
    "    return step_function(calculation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#순방향(Feed-forward) 신경망\n",
    "def sigmoid(t):  # 활성함수\n",
    "    return 1 / (1 + math.exp(-t))\n",
    "def neuron_output(weights, inputs):\n",
    "    return sigmoid(dot(weights, inputs))     # bias는 weights 리스트에 1을 더한 것으로 포함\n",
    "def feed_forward(neural_network, input_vector):\n",
    "    outputs = [] #뉴런의 값을 종합하는 결과리스트\n",
    "    \n",
    "    for layer in neural_network:   #한 층을 게산, neural_network는 각 weight를 가진 리스트\n",
    "        input_with_bias = input_vector + [1] # bias 추가\n",
    "        output = [neuron_output(neuron, input_with_bias) for neuron in layer] # 한 층에서 각 뉴런의 결과를 계산\n",
    "        outputs.append(output) # 한 층의 결과를 추가\n",
    "        \n",
    "        input_vector = output # 이전 층의 결과가 다음 층의 입력값이 된다.\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 [9.38314668300676e-14]\n",
      "0 1 [0.9999999999999059]\n",
      "1 0 [0.9999999999999059]\n",
      "1 1 [9.383146683006828e-14]\n"
     ]
    }
   ],
   "source": [
    "# xor 분리기\n",
    "xor_network = [ \n",
    "               [[20, 20, -30],\n",
    "               [20, 20, -10]],\n",
    "               [[-60,60,-30]]\n",
    "              ]\n",
    "for x in [0, 1]:\n",
    "    for y in [0, 1]:\n",
    "        print(x, y, feed_forward(xor_network, [x, y])[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagate(network, input_vector, targets):\n",
    "    #이전층의 출력과 다음층의 결과값\n",
    "    hidden_outputs, outputs = feed_forward(network, input_vector)\n",
    "    #sigmoid함수미분 * 오류(다음층계산값 - 실재결과)\n",
    "    output_deltas = [output * (1 - output) * (output - target) for output, target in zip(outputs, targets)] \n",
    "    #다음층 weight 갱신\n",
    "    for i, output_neuron in enumerate(network[-1]):\n",
    "        # i번째 뉴런에 대해\n",
    "        for j, hidden_output in enumerate(hidden_outputs + [1]):\n",
    "            # j번째 입력값과 오류값의 차이로 기울기 조금 기울임\n",
    "            output_neuron[j] -= output_deltas[i] * hidden_output \n",
    "    #이전층의 오류값을 뒤로 전파 \n",
    "    hidden_deltas = [hidden_output * (1 - hidden_output) * dot(output_deltas, [n[i] for n in output_layer]) \n",
    "                    for i, hidden_output in enumerate(hidden_outputs)]\n",
    "    #이전층의 weight 갱신\n",
    "    for i, hidden_neuron in enumerate(network[0]):\n",
    "        #i번째 뉴런에 대해\n",
    "        for j, input in enumerate(input_vector + [1]):\n",
    "            # j번째 입력값과 오류값의 차이로 기울기 조금 기울임\n",
    "            hidden_neuron[j] -= hidden_deltas[i] * input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_digits = [\n",
    "          \"\"\"11111\n",
    "             1...1\n",
    "             1...1\n",
    "             1...1\n",
    "             11111\"\"\",\n",
    "          \"\"\"..1..\n",
    "             ..1..\n",
    "             ..1..\n",
    "             ..1..\n",
    "             ..1..\"\"\",\n",
    "          \"\"\"11111\n",
    "             ....1\n",
    "             11111\n",
    "             1....\n",
    "             11111\"\"\",\n",
    "          \"\"\"11111\n",
    "             ....1\n",
    "             11111\n",
    "             ....1\n",
    "             11111\"\"\",\n",
    "          \"\"\"1...1\n",
    "             1...1\n",
    "             11111\n",
    "             ....1\n",
    "             ....1\"\"\",\n",
    "          \"\"\"11111\n",
    "             1....\n",
    "             11111\n",
    "             ....1\n",
    "             11111\"\"\",\n",
    "          \"\"\"11111\n",
    "             1....\n",
    "             11111\n",
    "             1...1\n",
    "             11111\"\"\",\n",
    "          \"\"\"11111\n",
    "             ....1\n",
    "             ....1\n",
    "             ....1\n",
    "             ....1\"\"\",\n",
    "          \"\"\"11111\n",
    "             1...1\n",
    "             11111\n",
    "             1...1\n",
    "             11111\"\"\",\n",
    "          \"\"\"11111\n",
    "             1...1\n",
    "             11111\n",
    "             ....1\n",
    "             11111\"\"\"]\n",
    "def make_digit(raw_digit):\n",
    "    return [1 if c == '1' else 0\n",
    "            for row in raw_digit.split(\"\\n\")\n",
    "            for c in row.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "input_size = 25\n",
    "num_hidden = 5\n",
    "output_size = 10\n",
    "targets = [[1 if i == j else 0 for i in range(10) for j in range(10)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.96, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.03, 0.0]\n",
      "1 [0.0, 0.96, 0.03, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "2 [0.0, 0.02, 0.96, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0]\n",
      "3 [0.0, 0.03, 0.0, 0.97, 0.0, 0.0, 0.0, 0.02, 0.0, 0.03]\n",
      "4 [0.0, 0.02, 0.02, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "5 [0.0, 0.0, 0.02, 0.0, 0.0, 0.96, 0.01, 0.0, 0.02, 0.01]\n",
      "6 [0.0, 0.0, 0.01, 0.0, 0.01, 0.01, 0.99, 0.0, 0.0, 0.0]\n",
      "7 [0.02, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.97, 0.0, 0.0]\n",
      "8 [0.03, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.96, 0.03]\n",
      "9 [0.0, 0.0, 0.0, 0.01, 0.0, 0.02, 0.0, 0.0, 0.03, 0.95]\n"
     ]
    }
   ],
   "source": [
    "def predict(input):\n",
    "        return feed_forward(network, input)[-1]\n",
    "\n",
    "inputs = list(map(make_digit, raw_digits))\n",
    "targets = [[1 if i == j else 0 for i in range(10)] for j in range(10)]\n",
    "\n",
    "#임의의 weight와 bias 초기화\n",
    "# 입력층->은닉층의 weight와 bias\n",
    "hidden_layer = [[random.random() for __ in range(input_size + 1)]\n",
    "                for __ in range(num_hidden)]\n",
    "# 은닉층->출력층의 weight와 bias\n",
    "output_layer = [[random.random() for __ in range(num_hidden + 1)]\n",
    "                for __ in range(output_size)]\n",
    "network = [hidden_layer, output_layer]\n",
    "\n",
    "#1만번 학습시행\n",
    "for __ in range(10000):\n",
    "    for input_vector, target_vector in zip(inputs, targets):\n",
    "        backpropagate(network, input_vector, target_vector)\n",
    "\n",
    "# 학습 데이터의 결과값 예측하기\n",
    "for i, input in enumerate(inputs):\n",
    "    outputs = predict(input)\n",
    "    print(i, [round(p, 2) for p in outputs])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
